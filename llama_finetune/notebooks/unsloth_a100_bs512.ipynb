{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ygazTxH3cDDM0OOAjHQBGN9RdP2qXsaE","timestamp":1746578079580},{"file_id":"1GIQApjroeDaxeCWcKpQEazPhr-5ECQEb","timestamp":1746559674239}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPc1JnjYE8AFtdY8wBk6nWR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b903d48902fa4d9885475f16bb5d21c8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_679bcc2ef43c4cc58ab981b5c7b59be0","IPY_MODEL_fd8805daf5de4f75b9ec00e4a71023d1","IPY_MODEL_4f015cee9f644ebf94c6c4f1b0364af7"],"layout":"IPY_MODEL_f19614886c78452ab074d03a104586ff"}},"679bcc2ef43c4cc58ab981b5c7b59be0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65f33e25901346759156437fd050d3f9","placeholder":"​","style":"IPY_MODEL_d3a0e2f8cbca4f77843e900552ada9d5","value":"Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"}},"fd8805daf5de4f75b9ec00e4a71023d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9065ef17a8f4165abeba26c4fd5931d","max":8356,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d10aea268fe440eb35ffe161c7d31f5","value":8356}},"4f015cee9f644ebf94c6c4f1b0364af7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e34d78e75f1f4367ac878d033a70fc2d","placeholder":"​","style":"IPY_MODEL_661eb22b0c394f91a8b0b5e66e93ce68","value":" 8356/8356 [00:14&lt;00:00, 691.71 examples/s]"}},"f19614886c78452ab074d03a104586ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65f33e25901346759156437fd050d3f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3a0e2f8cbca4f77843e900552ada9d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9065ef17a8f4165abeba26c4fd5931d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d10aea268fe440eb35ffe161c7d31f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e34d78e75f1f4367ac878d033a70fc2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"661eb22b0c394f91a8b0b5e66e93ce68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d80404ff7b3548cc8ccfc864848697ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9f424bf85bf488698f2ace23f726fc3","IPY_MODEL_a8d573c69521470ea37b88240105d460","IPY_MODEL_59c08d1ef7e147d4ac76f5fdb7bf6807"],"layout":"IPY_MODEL_f5a5954e212f40b881be01704f1db55f"}},"c9f424bf85bf488698f2ace23f726fc3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58868f2b4092407abd66bf8631e5426e","placeholder":"​","style":"IPY_MODEL_08fc5b1f5f79474ca0b1b78e331268eb","value":"Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"}},"a8d573c69521470ea37b88240105d460":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dcf1ac30d53448aa90caaba8cee00e0","max":929,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c494e0b207a45abb6f7b0cddce73518","value":929}},"59c08d1ef7e147d4ac76f5fdb7bf6807":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a39fa7afbee449ab914f9ee003358d2","placeholder":"​","style":"IPY_MODEL_fa8231d53a494c5f95254a20a6872a41","value":" 929/929 [00:03&lt;00:00, 313.10 examples/s]"}},"f5a5954e212f40b881be01704f1db55f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58868f2b4092407abd66bf8631e5426e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08fc5b1f5f79474ca0b1b78e331268eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9dcf1ac30d53448aa90caaba8cee00e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c494e0b207a45abb6f7b0cddce73518":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a39fa7afbee449ab914f9ee003358d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa8231d53a494c5f95254a20a6872a41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n","    !pip install --no-deps unsloth"],"metadata":{"id":"_SXN5DQWJ8Ro","executionInfo":{"status":"ok","timestamp":1746584719839,"user_tz":240,"elapsed":4618,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 512 # chosen for optimum results and training time\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","# 4bit pre quantized models\n","fourbit_models = [\n","    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n","    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n","    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n","    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n","    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n","    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n","    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n","    \"unsloth/Phi-3-medium-4k-instruct\",\n","    \"unsloth/gemma-2-9b-bnb-4bit\",\n","    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n","\n","    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n","    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n","    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n","    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n","\n","    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n","]\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Llama-3.2-1B-bnb-4bit\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CrLeXO4lg9yk","executionInfo":{"status":"ok","timestamp":1746584746919,"user_tz":240,"elapsed":27072,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"28077137-2f70-4862-f33f-6881b2d4ba24"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","🦥 Unsloth Zoo will now patch everything to make training faster!\n","==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n","   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]}]},{"cell_type":"code","source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16,\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # 0 is optimized\n","    bias = \"none\",    # \"none\" is optimized\n","    # \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # rank stabilized LoRA is set to false\n","    loftq_config = None, # And L\n",")"],"metadata":{"id":"l7TP-Je0hE7Z","executionInfo":{"status":"ok","timestamp":1746584752469,"user_tz":240,"elapsed":5551,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9c354d40-4bdc-4709-bda6-9c69c2449d85"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Unsloth 2025.4.7 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lltZzYYqhWxe","executionInfo":{"status":"ok","timestamp":1746584753198,"user_tz":240,"elapsed":728,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"a10d7179-42ce-4c87-8c57-ead477bbd505"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","txt_path = '/content/drive/MyDrive/HPML_Project/dataset/finance_corpus.txt'\n","\n","# load the finance_corpus.txt\n","ds = load_dataset(\n","    \"text\",\n","    data_files={\"train\": txt_path},\n","    split=\"train\",\n",")\n","ds = ds.filter(lambda x: x[\"text\"].strip() != \"\")\n","\n","print(f\"Loaded {len(ds)} examples; sample text:\")\n","print(ds[0][\"text\"][:200].replace(\"\\n\",\" \"), \"…\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_ALploEhXsp","executionInfo":{"status":"ok","timestamp":1746584753907,"user_tz":240,"elapsed":701,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"89d2a41d-c371-4c84-a15a-8ee22a60c50a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 227699 examples; sample text:\n","Article 1: Warren Buffett Autographed Books To Help Charity: Here's How You Can Get Legendary Investor's Signature …\n"]}]},{"cell_type":"code","source":["split = ds.train_test_split(test_size=0.10, seed=42)\n","train_ds = split[\"train\"]\n","val_ds   = split[\"test\"]"],"metadata":{"id":"lH4_lOyfL3V8","executionInfo":{"status":"ok","timestamp":1746584753915,"user_tz":240,"elapsed":4,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import re\n","from datasets import Dataset\n","\n","articles, buf = [], []\n","\n","with open(txt_path, encoding=\"utf‑8\") as f:\n","    for line in f:\n","        # new article header?\n","        if re.match(r\"^Article\\s+\\d+:\", line):\n","            if buf:\n","                articles.append(\" \".join(buf).strip())\n","                buf = []\n","        buf.append(line.strip())\n","    if buf:\n","        articles.append(\" \".join(buf).strip())\n","\n","print(\"Total articles:\", len(articles))\n","ds = Dataset.from_dict({\"text\": articles})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5mVgXwejUWG","executionInfo":{"status":"ok","timestamp":1746584754880,"user_tz":240,"elapsed":963,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"02aef108-69c0-424e-a8ec-b327d9be86d7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Total articles: 9285\n"]}]},{"cell_type":"code","source":["split = ds.train_test_split(test_size=0.10, seed=42)\n","train_ds = split[\"train\"]\n","val_ds   = split[\"test\"]"],"metadata":{"id":"LE9dD_dcjWQ2","executionInfo":{"status":"ok","timestamp":1746584754886,"user_tz":240,"elapsed":6,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_ds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vy--5gwdMc1l","executionInfo":{"status":"ok","timestamp":1746584754897,"user_tz":240,"elapsed":9,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"16a4cbf4-08cd-4ad1-80d0-30e730c2a48d"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['text'],\n","    num_rows: 8356\n","})"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["for text in ds[\"text\"][:10]:\n","    toks = tokenizer(text, add_special_tokens=False)\n","    print(len(toks[\"input_ids\"]), \"tokens\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K_9k-9GCKkvX","executionInfo":{"status":"ok","timestamp":1746584755010,"user_tz":240,"elapsed":28,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"6a3ae654-18fa-4853-8cf9-67075130dcef"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["670 tokens\n","799 tokens\n","696 tokens\n","554 tokens\n","797 tokens\n","813 tokens\n","728 tokens\n","757 tokens\n","962 tokens\n","974 tokens\n"]}]},{"cell_type":"code","source":["from transformers import TrainerCallback\n","import math\n","\n","class PerplexityCallback(TrainerCallback):\n","    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n","        if metrics is not None and \"eval_loss\" in metrics:\n","            ppl = math.exp(metrics[\"eval_loss\"])\n","            print(f\"Eval perplexity: {ppl:.2f}\")"],"metadata":{"id":"VtmreDNNJhA-","executionInfo":{"status":"ok","timestamp":1746584755053,"user_tz":240,"elapsed":42,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from trl import SFTTrainer\n","from transformers import TrainingArguments, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = train_ds,\n","    eval_dataset     = val_ds,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","\n","    # data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n","    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False),\n","    callbacks = [PerplexityCallback()],\n","    dataset_num_proc = 2,\n","    packing = True, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 64,\n","        gradient_accumulation_steps = 8,\n","        warmup_steps = 5,\n","        num_train_epochs = 5, # Set this for 1 full training run.\n","        # max_steps = 60,\n","        eval_strategy = \"epoch\",\n","        learning_rate = 2e-5,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 5,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        report_to = \"none\", # Use this for WandB etc\n","    ),\n",")"],"metadata":{"id":"vVo3DXWShZpk","executionInfo":{"status":"ok","timestamp":1746585083287,"user_tz":240,"elapsed":18891,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"colab":{"base_uri":"https://localhost:8080/","height":116,"referenced_widgets":["b903d48902fa4d9885475f16bb5d21c8","679bcc2ef43c4cc58ab981b5c7b59be0","fd8805daf5de4f75b9ec00e4a71023d1","4f015cee9f644ebf94c6c4f1b0364af7","f19614886c78452ab074d03a104586ff","65f33e25901346759156437fd050d3f9","d3a0e2f8cbca4f77843e900552ada9d5","e9065ef17a8f4165abeba26c4fd5931d","9d10aea268fe440eb35ffe161c7d31f5","e34d78e75f1f4367ac878d033a70fc2d","661eb22b0c394f91a8b0b5e66e93ce68","d80404ff7b3548cc8ccfc864848697ed","c9f424bf85bf488698f2ace23f726fc3","a8d573c69521470ea37b88240105d460","59c08d1ef7e147d4ac76f5fdb7bf6807","f5a5954e212f40b881be01704f1db55f","58868f2b4092407abd66bf8631e5426e","08fc5b1f5f79474ca0b1b78e331268eb","9dcf1ac30d53448aa90caaba8cee00e0","5c494e0b207a45abb6f7b0cddce73518","1a39fa7afbee449ab914f9ee003358d2","fa8231d53a494c5f95254a20a6872a41"]},"outputId":"acb37f3a-ab55-44d2-8bea-b121b31ce053"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/8356 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b903d48902fa4d9885475f16bb5d21c8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Unsloth: Hugging Face's packing is currently buggy - we're disabling it for now!\n"]},{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/929 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80404ff7b3548cc8ccfc864848697ed"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Unsloth: Hugging Face's packing is currently buggy - we're disabling it for now!\n"]}]},{"cell_type":"code","source":["tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"baAtb0tawBMs","executionInfo":{"status":"ok","timestamp":1746585098335,"user_tz":240,"elapsed":3,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"3fdbe287-6f1e-4ec9-dc22-c3bceb5a8437"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|begin_of_text|>Article 3263: Breaking News: Dow, S&amp;P, Nasdaq Futures Tumble Ahead of Big Earnings Week  April 21 - U.S. equity futures declined on Monday, as investors returned from the holiday weekend facing renewed concerns over U.S.-China trade tensions and a packed earnings calendar.  Nasdaq 100 futures slid about 1.2%, while Dow Jones Industrial Average and S&P 500 futures dropped nearly 0.9% and 1.1%, respectively. The three major indices also ended last week lower, notching their third loss in four weeks.  Weighing on sentiment was UnitedHealth (NYSE:UNH), which fell more than 22% on Thursday after cutting its full-year outlook and reporting underwhelming earnings. In contrast, Eli Lilly (NYSE:LLY) surged 14% following positive late-stage trial results for its experimental weight loss drug, orforglipron.  The ongoing lack of progress in direct U.S.-China trade talks, along with comments from Chicago Fed President Austan Goolsbee warning of potential economic slowdown from tariffs, added to the cautious tone.  Investors are bracing for key data releases this week, including durable goods orders and PMI readings, along with earnings from more than 100 S&P 500 firms. Notables include Alphabet (NASDAQ:GOOG), Tesla (NASDAQ:TSLA), Verizon (NYSE:VZ), and Procter & Gamble (NYSE:PG).  Meanwhile, the 10-year U.S. Treasury yield hovered near 4.36%, and WTI crude futures slipped toward $62.97 per barrel.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["trainer.train_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JS5u6_owwpnO","executionInfo":{"status":"ok","timestamp":1746585100429,"user_tz":240,"elapsed":3,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"5b9dc14a-c220-4421-8c95-168985198a78"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['text', 'input_ids', 'attention_mask'],\n","    num_rows: 8356\n","})"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["trainer_stats = trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"xWbhg-Vswn1C","outputId":"6fecf7a0-d177-4533-8f41-8f150338c009","executionInfo":{"status":"ok","timestamp":1746586193782,"user_tz":240,"elapsed":1090670,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}}},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 8,356 | Num Epochs = 5 | Total steps = 80\n","O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 8\n","\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 8 x 1) = 512\n"," \"-____-\"     Trainable parameters = 11,272,192/1,000,000,000 (1.13% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [80/80 17:55, Epoch 4/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.374000</td>\n","      <td>2.380733</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.336600</td>\n","      <td>2.328353</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.277500</td>\n","      <td>2.300270</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.265800</td>\n","      <td>2.282648</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Eval perplexity: 10.81\n","Eval perplexity: 10.26\n","Eval perplexity: 9.98\n","Eval perplexity: 9.83\n","Eval perplexity: 9.80\n"]}]},{"cell_type":"code","source":["FastLanguageModel.for_inference(model)\n","\n","def answer(prompt: str,\n","           max_new_tokens: int = 128,\n","           temperature: float    = 0.2,\n","           top_p: float          = 0.7,\n","           repetition_penalty: float = 1.2,\n","           no_repeat_ngram_size: int = 3):\n","    # 1) tokenize\n","    inputs = tokenizer(\n","        prompt,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=512\n","    ).to(model.device)\n","\n","    input_ids = inputs[\"input_ids\"]\n","\n","    # 2) generate with anti‐repetition tweaks\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens       = max_new_tokens,\n","        temperature          = temperature,\n","        top_p                = top_p,\n","        do_sample            = True,\n","        repetition_penalty   = repetition_penalty,\n","        no_repeat_ngram_size = no_repeat_ngram_size,\n","        eos_token_id         = tokenizer.eos_token_id,\n","        pad_token_id         = tokenizer.pad_token_id,\n","        early_stopping       = True,\n","    )\n","\n","    # 3) strip off prompt‐tokens and decode only the new ones\n","    gen_ids = outputs[0][ input_ids.shape[-1] : ]\n","    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n","\n","# Try it out\n","print(answer(\"How is starbucks doing?\"))"],"metadata":{"id":"NDoSjeazw_3c","executionInfo":{"status":"ok","timestamp":1746586954659,"user_tz":240,"elapsed":4041,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d9fab489-009d-45ac-8272-9351da31f485"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Starbucks Corporation (NASDAQ:SBUX) has been a popular stock among investors, with its shares trading at $90.00 as of Friday’s close. The company reported earnings per share in the first quarter that beat analysts’ expectations by 1%. This was due to strong sales growth and increased operating margins.\n","Starbucks also announced plans for an expansion into new markets such as China and India. These moves are expected to help increase revenue from these regions significantly over time. Additionally, they plan on investing heavily in technology infrastructure which could lead to further improvements in customer experience moving forward too!\n","In addition to this news release there were other positive developments\n"]}]},{"cell_type":"code","source":["save_path = \"/content/drive/MyDrive/HPML_Project/copy_unsloth_a100_2\"   # <— adjust if your folder is nested\n","\n","# 1) Save LoRA adapter + config\n","trainer.save_model(save_path)\n","\n","# 2) Save tokenizer files\n","tokenizer.save_pretrained(save_path)\n","\n","print(\" Saved adapters + tokenizer to\", save_path)"],"metadata":{"id":"4r2O9D3fxU-E","executionInfo":{"status":"ok","timestamp":1746586976619,"user_tz":240,"elapsed":2196,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8fea42d8-14b5-4e3f-9203-ca031ad42081"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":[" Saved adapters + tokenizer to /content/drive/MyDrive/HPML_Project/copy_unsloth_a100_2\n"]}]},{"cell_type":"markdown","source":["## Inference\n"],"metadata":{"id":"s23uOdZsCjLN"}},{"cell_type":"code","source":["# inference.py\n","\n","import torch\n","from peft import prepare_model_for_kbit_training, PeftModel\n","from unsloth import FastLanguageModel\n","\n","# 1) Load the same 4-bit base + tokenizer you fine-tuned on\n","base, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name     = \"unsloth/Llama-3.2-1B-bnb-4bit\",  # your base\n","    max_seq_length = 128,\n","    dtype          = torch.float16,                   # or None for auto\n","    load_in_4bit   = True,\n","    device_map     = \"auto\",\n",")\n","\n","# ensure pad/eos tokens are set\n","tokenizer.pad_token = tokenizer.eos_token\n","base.config.pad_token_id = tokenizer.pad_token_id\n","base.config.use_cache      = True\n","\n","# 2) Patch for QLoRA / k-bit adapters\n","base = prepare_model_for_kbit_training(base)\n","\n","# 3) Load your fine-tuned LoRA adapters\n","model = PeftModel.from_pretrained(\n","    base,\n","    \"/content/drive/MyDrive/HPML_Project/copy_unsloth_a100_2\",     # folder where you saved adapters + tokenizer\n","    device_map=\"auto\",          # shard onto GPU automatically\n",")\n","\n","# model.eval()\n","FastLanguageModel.for_inference(model)\n","\n","# 4) Inference helper\n","def answer(prompt: str,\n","           max_new_tokens: int = 128,\n","           temperature: float    = 0.2,\n","           top_p: float          = 0.7,\n","           repetition_penalty: float = 1.2,\n","           no_repeat_ngram_size: int = 3):\n","    # 1) tokenize\n","    inputs = tokenizer(\n","        prompt,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=512\n","    ).to(model.device)\n","\n","    input_ids = inputs[\"input_ids\"]\n","\n","    # 2) generate with anti‐repetition tweaks\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens       = max_new_tokens,\n","        temperature          = temperature,\n","        top_p                = top_p,\n","        do_sample            = True,\n","        repetition_penalty   = repetition_penalty,\n","        no_repeat_ngram_size = no_repeat_ngram_size,\n","        eos_token_id         = tokenizer.eos_token_id,\n","        pad_token_id         = tokenizer.pad_token_id,\n","        early_stopping       = True,\n","    )\n","\n","    # 3) strip off prompt‐tokens and decode only the new ones\n","    gen_ids = outputs[0][ input_ids.shape[-1] : ]\n","    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n","\n","# Try it out\n","print(answer(\"How is starbucks stock doing?\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VH9QPCYFCiJv","executionInfo":{"status":"ok","timestamp":1746587126926,"user_tz":240,"elapsed":14768,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"9e174e12-eb77-4053-c7e0-859d81bcece4"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n","   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Starbucks Corporation (SBUX) has been a popular choice among investors looking to capitalize on the company’s strong performance. The coffee giant recently reported its third-quarter earnings, which showed impressive growth in revenue and net income compared with last year.\n","Starbucks’ latest results were driven by robust sales of premium products such as lattes and frappuccinos along with increased spending from customers who are more willing to spend money during uncertain times due to inflationary pressures. This increase in consumer demand led them to invest heavily into new stores opening up across North America while also expanding their digital presence through mobile apps like Apple Pay or Google Wallet allowing people access\n"]}]},{"cell_type":"code","source":["# inference.py\n","\n","import torch\n","from peft import prepare_model_for_kbit_training, PeftModel\n","from unsloth import FastLanguageModel\n","\n","# 1) Load the same 4-bit base + tokenizer you fine-tuned on\n","base, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name     = \"unsloth/Llama-3.2-1B-bnb-4bit\",  # your base\n","    max_seq_length = 128,\n","    dtype          = torch.float16,                   # or None for auto\n","    load_in_4bit   = True,\n","    device_map     = \"auto\",\n",")\n","\n","# ensure pad/eos tokens are set\n","tokenizer.pad_token = tokenizer.eos_token\n","base.config.pad_token_id = tokenizer.pad_token_id\n","base.config.use_cache      = True\n","\n","# 2) Patch for QLoRA / k-bit adapters\n","base = prepare_model_for_kbit_training(base)\n","\n","# 3) Load your fine-tuned LoRA adapters\n","model = PeftModel.from_pretrained(\n","    base,\n","    \"/content/drive/MyDrive/HPML_Project/unsloth_a100_2\",     # folder where you saved adapters + tokenizer\n","    device_map=\"auto\",          # shard onto GPU automatically\n",")\n","\n","# model.eval()\n","FastLanguageModel.for_inference(model)\n","\n","# 4) Inference helper\n","def answer(prompt: str,\n","           max_new_tokens: int = 128,\n","           temperature: float    = 0.2,\n","           top_p: float          = 0.9,\n","           repetition_penalty: float = 1.2,\n","           no_repeat_ngram_size: int = 3):\n","    # 1) tokenize\n","    inputs = tokenizer(\n","        prompt,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=512\n","    ).to(model.device)\n","\n","    input_ids = inputs[\"input_ids\"]\n","\n","    # 2) generate with anti‐repetition tweaks\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens       = max_new_tokens,\n","        temperature          = temperature,\n","        top_p                = top_p,\n","        do_sample            = True,\n","        repetition_penalty   = repetition_penalty,\n","        no_repeat_ngram_size = no_repeat_ngram_size,\n","        eos_token_id         = tokenizer.eos_token_id,\n","        pad_token_id         = tokenizer.pad_token_id,\n","        early_stopping       = True,\n","    )\n","\n","    # 3) strip off prompt‐tokens and decode only the new ones\n","    gen_ids = outputs[0][ input_ids.shape[-1] : ]\n","    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n","\n","# Try it out\n","print(answer(\"What is the best performing stock?\"))"],"metadata":{"id":"djMnOyY1CyMv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746587209464,"user_tz":240,"elapsed":15238,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"141a8be4-a6a5-4c02-9876-47a2f84d32d7"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n","   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Find out here >>Continue Reading about Is Now The Time To Buy These 3 Stocks With High Dividend Yields And Low P/E Ratios? Here’s What Analysts Are Saying. Continue Reading about Why You Should Be Buying This Stock Right Now: It Has A Strong Growth Story, But Its Price Isn’t Cheap!>> Read this article to know more... continue-the-read   1 day ago 02:30 PM EDT 04:00 AM PDT Monday March 18, 2024 JPMorgan Chase & Co (NYSE:JPM) has been a strong performer in recent years and currently trades at $93.\n"]}]},{"cell_type":"code","source":["# inference.py\n","\n","import torch\n","from peft import prepare_model_for_kbit_training, PeftModel\n","from unsloth import FastLanguageModel\n","\n","# 1) Load the same 4-bit base + tokenizer you fine-tuned on\n","base, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name     = \"unsloth/Llama-3.2-1B-bnb-4bit\",  # your base\n","    max_seq_length = 128,\n","    dtype          = torch.float16,                   # or None for auto\n","    load_in_4bit   = True,\n","    device_map     = \"auto\",\n",")\n","\n","# ensure pad/eos tokens are set\n","tokenizer.pad_token = tokenizer.eos_token\n","base.config.pad_token_id = tokenizer.pad_token_id\n","base.config.use_cache      = True\n","\n","# 2) Patch for QLoRA / k-bit adapters\n","base = prepare_model_for_kbit_training(base)\n","\n","# 3) Load your fine-tuned LoRA adapters\n","model = PeftModel.from_pretrained(\n","    base,\n","    \"/content/drive/MyDrive/HPML_Project/unsloth_a100_2\",     # folder where you saved adapters + tokenizer\n","    device_map=\"auto\",          # shard onto GPU automatically\n",")\n","\n","# model.eval()\n","FastLanguageModel.for_inference(model)\n","\n","# 4) Inference helper\n","def answer(prompt: str,\n","           max_new_tokens: int = 128,\n","           temperature: float    = 0.2,\n","           top_p: float          = 0.9,\n","           repetition_penalty: float = 1.2,\n","           no_repeat_ngram_size: int = 3):\n","    # 1) tokenize\n","    inputs = tokenizer(\n","        prompt,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=512\n","    ).to(model.device)\n","\n","    input_ids = inputs[\"input_ids\"]\n","\n","    # 2) generate with anti‐repetition tweaks\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens       = max_new_tokens,\n","        temperature          = temperature,\n","        top_p                = top_p,\n","        do_sample            = True,\n","        repetition_penalty   = repetition_penalty,\n","        no_repeat_ngram_size = no_repeat_ngram_size,\n","        eos_token_id         = tokenizer.eos_token_id,\n","        pad_token_id         = tokenizer.pad_token_id,\n","        early_stopping       = True,\n","    )\n","\n","    # 3) strip off prompt‐tokens and decode only the new ones\n","    gen_ids = outputs[0][ input_ids.shape[-1] : ]\n","    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n","\n","# Try it out\n","print(answer(\"What is the news on Lockheed Martin?\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FE_anQYVdGfv","executionInfo":{"status":"ok","timestamp":1746587279218,"user_tz":240,"elapsed":15026,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"95ce9ff0-727d-4f56-e921-ddcae3bf1ab8"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n","   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","The company has been making progress in its efforts to improve profitability and reduce costs. It recently announced a $1 billion cost reduction plan, which includes reducing capital spending by 10% over three years while maintaining investments for future growth opportunities. This move should help boost earnings per share (EPS) significantly as it reduces expenses without sacrificing quality or innovation. Additionally, management expects revenue from defense systems contracts to grow at an average annual rate of around 6%. These positive developments suggest that investors may be optimistic about this stock's prospects going forward. However, there are also concerns regarding potential tariff impacts due to recent trade tensions between China and other countries.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"TKTjvdjwezAA"},"execution_count":null,"outputs":[]}]}