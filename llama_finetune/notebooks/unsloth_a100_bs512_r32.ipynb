{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-g4_6Q2fj2GWDOVE4Bb0blJ2VY_lMHxe","timestamp":1746587417385},{"file_id":"1ygazTxH3cDDM0OOAjHQBGN9RdP2qXsaE","timestamp":1746578079580},{"file_id":"1GIQApjroeDaxeCWcKpQEazPhr-5ECQEb","timestamp":1746559674239}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPjaG8RpEdTu2v5R9f5c+V6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"37564e615d6f4b9daed529604044d29f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7de1e1a4692c42a4b00db70ca57aced3","IPY_MODEL_0bb6e1d29ae34bc88afea17cc5a4fed2","IPY_MODEL_53ce86ee22ed4363aeda3b2e0eddb02a"],"layout":"IPY_MODEL_584a922244f04e91953c7d62be1ed6e2"}},"7de1e1a4692c42a4b00db70ca57aced3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9072d19eab474a60ad5e375741ab8fa5","placeholder":"​","style":"IPY_MODEL_9250f4b50c974f949dc58179f237b614","value":"Generating train split: "}},"0bb6e1d29ae34bc88afea17cc5a4fed2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_181357a9dc354e4c95499ba268701dbc","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c42e9d739f9642adae532b76b35e5117","value":1}},"53ce86ee22ed4363aeda3b2e0eddb02a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1a8f116be7a477eb228692d7939c927","placeholder":"​","style":"IPY_MODEL_62461229c9eb4679bd02513c015dec45","value":" 441891/0 [00:01&lt;00:00, 275396.64 examples/s]"}},"584a922244f04e91953c7d62be1ed6e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9072d19eab474a60ad5e375741ab8fa5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9250f4b50c974f949dc58179f237b614":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"181357a9dc354e4c95499ba268701dbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c42e9d739f9642adae532b76b35e5117":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b1a8f116be7a477eb228692d7939c927":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62461229c9eb4679bd02513c015dec45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33a7bad1c3c24defabdb154258db93ff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_577e73cbf1b14881b6d8d5ae328c4243","IPY_MODEL_b85a09ebd8b643aa9d2dbf2e145817e0","IPY_MODEL_13939af6e52a45e8a64e68f13d19372f"],"layout":"IPY_MODEL_04934fdca8ff4f30b40ee7123d5168d5"}},"577e73cbf1b14881b6d8d5ae328c4243":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_777da9a6d0f449f28e4ccc36b1f91d36","placeholder":"​","style":"IPY_MODEL_39d74f5b149a46f39f62b91a346608ed","value":"Filter: 100%"}},"b85a09ebd8b643aa9d2dbf2e145817e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddd52729b0df41a78654d2e5f54ba221","max":441891,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3dfc7aa0ba8a43b4b551f6a03864da85","value":441891}},"13939af6e52a45e8a64e68f13d19372f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5eece04a686546bab21ef5ef62b62c6c","placeholder":"​","style":"IPY_MODEL_44832868f8384316826be26f588db828","value":" 441891/441891 [00:01&lt;00:00, 438424.75 examples/s]"}},"04934fdca8ff4f30b40ee7123d5168d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"777da9a6d0f449f28e4ccc36b1f91d36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39d74f5b149a46f39f62b91a346608ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ddd52729b0df41a78654d2e5f54ba221":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dfc7aa0ba8a43b4b551f6a03864da85":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5eece04a686546bab21ef5ef62b62c6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44832868f8384316826be26f588db828":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e48186beba848f99e155b5c1ff0f0b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_045a23c9929c446c8b0a5a12ed7f2958","IPY_MODEL_64441d058612487bab25f74e918a294e","IPY_MODEL_70a1096c9a744c5da59c435a1d4bf20b"],"layout":"IPY_MODEL_e757d784e6504ba1aa1961d4f63869f9"}},"045a23c9929c446c8b0a5a12ed7f2958":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6ade361409948a99d58143743b6f029","placeholder":"​","style":"IPY_MODEL_8e45196fbc27477b84d1246fa9846115","value":"Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"}},"64441d058612487bab25f74e918a294e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1acae4539ccf4e59baeb3c29f2b9f025","max":8356,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8bb3e7d48eda4256b4096a602fecc326","value":8356}},"70a1096c9a744c5da59c435a1d4bf20b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_437c915643004b5cb21267ad59b2ceb8","placeholder":"​","style":"IPY_MODEL_016661bdaa844f4bb4ed3e1b464a2edf","value":" 8356/8356 [00:16&lt;00:00, 628.80 examples/s]"}},"e757d784e6504ba1aa1961d4f63869f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6ade361409948a99d58143743b6f029":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e45196fbc27477b84d1246fa9846115":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1acae4539ccf4e59baeb3c29f2b9f025":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bb3e7d48eda4256b4096a602fecc326":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"437c915643004b5cb21267ad59b2ceb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"016661bdaa844f4bb4ed3e1b464a2edf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e0ad08a45db4f46810ef77fedeab420":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11011d78fe6944a980ecd7017a8da512","IPY_MODEL_906c9eaa7eab432b92ab6d24bc5dbb01","IPY_MODEL_6df799b7a84f4d11bf5aea8f48c94874"],"layout":"IPY_MODEL_41b2cd4bc6d44e94a3255e57a5e9cabb"}},"11011d78fe6944a980ecd7017a8da512":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87825372fc0e4d3baa9ec315ec446e24","placeholder":"​","style":"IPY_MODEL_8443e0130e604e4f9e648e55c9e9ede7","value":"Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"}},"906c9eaa7eab432b92ab6d24bc5dbb01":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b258e94cd254cba856afcd219e54a13","max":929,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97accebc04614b5690321e8b600edfca","value":929}},"6df799b7a84f4d11bf5aea8f48c94874":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a0659c648f24454ba9dce304ce8e1d8","placeholder":"​","style":"IPY_MODEL_8ff06c3faff64ba09a09f84cb24ca9a5","value":" 929/929 [00:04&lt;00:00, 270.74 examples/s]"}},"41b2cd4bc6d44e94a3255e57a5e9cabb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87825372fc0e4d3baa9ec315ec446e24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8443e0130e604e4f9e648e55c9e9ede7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b258e94cd254cba856afcd219e54a13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97accebc04614b5690321e8b600edfca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a0659c648f24454ba9dce304ce8e1d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ff06c3faff64ba09a09f84cb24ca9a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n","    !pip install --no-deps unsloth"],"metadata":{"id":"_SXN5DQWJ8Ro","executionInfo":{"status":"ok","timestamp":1746587975396,"user_tz":240,"elapsed":14062,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 512 # chosen for optimum results and training time\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","# 4bit pre quantized models\n","fourbit_models = [\n","    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n","    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n","    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n","    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n","    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n","    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n","    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n","    \"unsloth/Phi-3-medium-4k-instruct\",\n","    \"unsloth/gemma-2-9b-bnb-4bit\",\n","    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n","\n","    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n","    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n","    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n","    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n","\n","    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n","]\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Llama-3.2-1B-bnb-4bit\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CrLeXO4lg9yk","executionInfo":{"status":"ok","timestamp":1746588163947,"user_tz":240,"elapsed":5530,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"4c02d7b6-5006-4306-c98c-f6a6b7057416"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n","   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]}]},{"cell_type":"code","source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 32,\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 32,\n","    lora_dropout = 0, # 0 is optimized\n","    bias = \"none\",    # \"none\" is optimized\n","    # \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # rank stabilized LoRA is set to false\n","    loftq_config = None, # And L\n",")"],"metadata":{"id":"l7TP-Je0hE7Z","executionInfo":{"status":"ok","timestamp":1746588169887,"user_tz":240,"elapsed":5939,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lltZzYYqhWxe","executionInfo":{"status":"ok","timestamp":1746588170384,"user_tz":240,"elapsed":498,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"b55d052e-1027-497d-cf68-a3c779f9a22c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","txt_path = '/content/drive/MyDrive/HPML_Project/dataset/finance_corpus.txt'\n","\n","# load the finance_corpus.txt\n","ds = load_dataset(\n","    \"text\",\n","    data_files={\"train\": txt_path},\n","    split=\"train\",\n",")\n","ds = ds.filter(lambda x: x[\"text\"].strip() != \"\")\n","\n","print(f\"Loaded {len(ds)} examples; sample text:\")\n","print(ds[0][\"text\"][:200].replace(\"\\n\",\" \"), \"…\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":116,"referenced_widgets":["37564e615d6f4b9daed529604044d29f","7de1e1a4692c42a4b00db70ca57aced3","0bb6e1d29ae34bc88afea17cc5a4fed2","53ce86ee22ed4363aeda3b2e0eddb02a","584a922244f04e91953c7d62be1ed6e2","9072d19eab474a60ad5e375741ab8fa5","9250f4b50c974f949dc58179f237b614","181357a9dc354e4c95499ba268701dbc","c42e9d739f9642adae532b76b35e5117","b1a8f116be7a477eb228692d7939c927","62461229c9eb4679bd02513c015dec45","33a7bad1c3c24defabdb154258db93ff","577e73cbf1b14881b6d8d5ae328c4243","b85a09ebd8b643aa9d2dbf2e145817e0","13939af6e52a45e8a64e68f13d19372f","04934fdca8ff4f30b40ee7123d5168d5","777da9a6d0f449f28e4ccc36b1f91d36","39d74f5b149a46f39f62b91a346608ed","ddd52729b0df41a78654d2e5f54ba221","3dfc7aa0ba8a43b4b551f6a03864da85","5eece04a686546bab21ef5ef62b62c6c","44832868f8384316826be26f588db828"]},"id":"5_ALploEhXsp","executionInfo":{"status":"ok","timestamp":1746588097605,"user_tz":240,"elapsed":4617,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"e6850144-03e9-4cc9-b055-c123c25efda2"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37564e615d6f4b9daed529604044d29f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Filter:   0%|          | 0/441891 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33a7bad1c3c24defabdb154258db93ff"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded 227699 examples; sample text:\n","Article 1: Warren Buffett Autographed Books To Help Charity: Here's How You Can Get Legendary Investor's Signature …\n"]}]},{"cell_type":"code","source":["split = ds.train_test_split(test_size=0.10, seed=42)\n","train_ds = split[\"train\"]\n","val_ds   = split[\"test\"]"],"metadata":{"id":"lH4_lOyfL3V8","executionInfo":{"status":"ok","timestamp":1746588180981,"user_tz":240,"elapsed":7,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["import re\n","from datasets import Dataset\n","\n","articles, buf = [], []\n","\n","with open(txt_path, encoding=\"utf‑8\") as f:\n","    for line in f:\n","        # new article header?\n","        if re.match(r\"^Article\\s+\\d+:\", line):\n","            if buf:\n","                articles.append(\" \".join(buf).strip())\n","                buf = []\n","        buf.append(line.strip())\n","    if buf:\n","        articles.append(\" \".join(buf).strip())\n","\n","print(\"Total articles:\", len(articles))\n","ds = Dataset.from_dict({\"text\": articles})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5mVgXwejUWG","executionInfo":{"status":"ok","timestamp":1746588183913,"user_tz":240,"elapsed":950,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"06e3f909-886c-45df-85bd-1cc783686de5"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Total articles: 9285\n"]}]},{"cell_type":"code","source":["split = ds.train_test_split(test_size=0.10, seed=42)\n","train_ds = split[\"train\"]\n","val_ds   = split[\"test\"]"],"metadata":{"id":"LE9dD_dcjWQ2","executionInfo":{"status":"ok","timestamp":1746588189111,"user_tz":240,"elapsed":5,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["train_ds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vy--5gwdMc1l","executionInfo":{"status":"ok","timestamp":1746588191387,"user_tz":240,"elapsed":4,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"7d2814b0-03d9-422d-bde8-7b6bed4f2264"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['text'],\n","    num_rows: 8356\n","})"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["for text in ds[\"text\"][:10]:\n","    toks = tokenizer(text, add_special_tokens=False)\n","    print(len(toks[\"input_ids\"]), \"tokens\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K_9k-9GCKkvX","executionInfo":{"status":"ok","timestamp":1746588193045,"user_tz":240,"elapsed":39,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"0ad051d1-f4bd-460e-d0ad-689cbcd4e4fd"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["670 tokens\n","799 tokens\n","696 tokens\n","554 tokens\n","797 tokens\n","813 tokens\n","728 tokens\n","757 tokens\n","962 tokens\n","974 tokens\n"]}]},{"cell_type":"code","source":["from transformers import TrainerCallback\n","import math\n","\n","class PerplexityCallback(TrainerCallback):\n","    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n","        if metrics is not None and \"eval_loss\" in metrics:\n","            ppl = math.exp(metrics[\"eval_loss\"])\n","            print(f\"Eval perplexity: {ppl:.2f}\")"],"metadata":{"id":"VtmreDNNJhA-","executionInfo":{"status":"ok","timestamp":1746588194670,"user_tz":240,"elapsed":2,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["from trl import SFTTrainer\n","from transformers import TrainingArguments, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = train_ds,\n","    eval_dataset     = val_ds,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","\n","    # data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n","    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False),\n","    callbacks = [PerplexityCallback()],\n","    dataset_num_proc = 2,\n","    packing = True, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 64,\n","        gradient_accumulation_steps = 8,\n","        warmup_steps = 5,\n","        num_train_epochs = 5, # Set this for 1 full training run.\n","        # max_steps = 60,\n","        eval_strategy = \"epoch\",\n","        learning_rate = 2e-5,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 5,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        report_to = \"none\", # Use this for WandB etc\n","    ),\n",")"],"metadata":{"id":"vVo3DXWShZpk","executionInfo":{"status":"ok","timestamp":1746588227517,"user_tz":240,"elapsed":21418,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"colab":{"base_uri":"https://localhost:8080/","height":116,"referenced_widgets":["4e48186beba848f99e155b5c1ff0f0b1","045a23c9929c446c8b0a5a12ed7f2958","64441d058612487bab25f74e918a294e","70a1096c9a744c5da59c435a1d4bf20b","e757d784e6504ba1aa1961d4f63869f9","a6ade361409948a99d58143743b6f029","8e45196fbc27477b84d1246fa9846115","1acae4539ccf4e59baeb3c29f2b9f025","8bb3e7d48eda4256b4096a602fecc326","437c915643004b5cb21267ad59b2ceb8","016661bdaa844f4bb4ed3e1b464a2edf","7e0ad08a45db4f46810ef77fedeab420","11011d78fe6944a980ecd7017a8da512","906c9eaa7eab432b92ab6d24bc5dbb01","6df799b7a84f4d11bf5aea8f48c94874","41b2cd4bc6d44e94a3255e57a5e9cabb","87825372fc0e4d3baa9ec315ec446e24","8443e0130e604e4f9e648e55c9e9ede7","1b258e94cd254cba856afcd219e54a13","97accebc04614b5690321e8b600edfca","8a0659c648f24454ba9dce304ce8e1d8","8ff06c3faff64ba09a09f84cb24ca9a5"]},"outputId":"056100bb-597a-4afa-eb12-5ce638aadb47"},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/8356 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e48186beba848f99e155b5c1ff0f0b1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Unsloth: Hugging Face's packing is currently buggy - we're disabling it for now!\n"]},{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/929 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0ad08a45db4f46810ef77fedeab420"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Unsloth: Hugging Face's packing is currently buggy - we're disabling it for now!\n"]}]},{"cell_type":"code","source":["tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"baAtb0tawBMs","executionInfo":{"status":"ok","timestamp":1746588232320,"user_tz":240,"elapsed":28,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"ed7471cd-3c24-4f6f-9cb6-402e38b25447"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|begin_of_text|>Article 3263: Breaking News: Dow, S&amp;P, Nasdaq Futures Tumble Ahead of Big Earnings Week  April 21 - U.S. equity futures declined on Monday, as investors returned from the holiday weekend facing renewed concerns over U.S.-China trade tensions and a packed earnings calendar.  Nasdaq 100 futures slid about 1.2%, while Dow Jones Industrial Average and S&P 500 futures dropped nearly 0.9% and 1.1%, respectively. The three major indices also ended last week lower, notching their third loss in four weeks.  Weighing on sentiment was UnitedHealth (NYSE:UNH), which fell more than 22% on Thursday after cutting its full-year outlook and reporting underwhelming earnings. In contrast, Eli Lilly (NYSE:LLY) surged 14% following positive late-stage trial results for its experimental weight loss drug, orforglipron.  The ongoing lack of progress in direct U.S.-China trade talks, along with comments from Chicago Fed President Austan Goolsbee warning of potential economic slowdown from tariffs, added to the cautious tone.  Investors are bracing for key data releases this week, including durable goods orders and PMI readings, along with earnings from more than 100 S&P 500 firms. Notables include Alphabet (NASDAQ:GOOG), Tesla (NASDAQ:TSLA), Verizon (NYSE:VZ), and Procter & Gamble (NYSE:PG).  Meanwhile, the 10-year U.S. Treasury yield hovered near 4.36%, and WTI crude futures slipped toward $62.97 per barrel.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["trainer.train_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JS5u6_owwpnO","executionInfo":{"status":"ok","timestamp":1746588235951,"user_tz":240,"elapsed":4,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"44a8f094-6b94-40a6-d89a-bdae392ab419"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['text', 'input_ids', 'attention_mask'],\n","    num_rows: 8356\n","})"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["trainer_stats = trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"id":"xWbhg-Vswn1C","outputId":"0cdee578-4c70-49a0-a2cb-c4dbc8a230e3","executionInfo":{"status":"ok","timestamp":1746589332552,"user_tz":240,"elapsed":1095037,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}}},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 8,356 | Num Epochs = 5 | Total steps = 80\n","O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 8\n","\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 8 x 1) = 512\n"," \"-____-\"     Trainable parameters = 22,544,384/1,000,000,000 (2.25% trained)\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Will smartly offload gradients to save VRAM!\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [80/80 17:42, Epoch 4/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.416000</td>\n","      <td>2.410909</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.350400</td>\n","      <td>2.332800</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.269600</td>\n","      <td>2.290567</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.249900</td>\n","      <td>2.268023</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n","Using gradient accumulation will be very slightly less accurate.\n","Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"]},{"output_type":"stream","name":"stdout","text":["Eval perplexity: 11.14\n","Eval perplexity: 10.31\n","Eval perplexity: 9.88\n","Eval perplexity: 9.70\n","Eval perplexity: 9.66\n"]}]},{"cell_type":"code","source":["FastLanguageModel.for_inference(model)\n","\n","def answer(prompt: str,\n","           max_new_tokens: int = 128,\n","           temperature: float    = 0.2,\n","           top_p: float          = 0.7,\n","           repetition_penalty: float = 1.2,\n","           no_repeat_ngram_size: int = 3):\n","    # 1) tokenize\n","    inputs = tokenizer(\n","        prompt,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=512\n","    ).to(model.device)\n","\n","    input_ids = inputs[\"input_ids\"]\n","\n","    # 2) generate with anti‐repetition tweaks\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens       = max_new_tokens,\n","        temperature          = temperature,\n","        top_p                = top_p,\n","        do_sample            = True,\n","        repetition_penalty   = repetition_penalty,\n","        no_repeat_ngram_size = no_repeat_ngram_size,\n","        eos_token_id         = tokenizer.eos_token_id,\n","        pad_token_id         = tokenizer.pad_token_id,\n","        early_stopping       = True,\n","    )\n","\n","    # 3) strip off prompt‐tokens and decode only the new ones\n","    gen_ids = outputs[0][ input_ids.shape[-1] : ]\n","    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n","\n","# Try it out\n","print(answer(\"How is starbucks doing?\"))"],"metadata":{"id":"NDoSjeazw_3c","executionInfo":{"status":"ok","timestamp":1746589364283,"user_tz":240,"elapsed":4045,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"425af598-649f-4ba5-f7d4-16d51a2c1d2d"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Starbucks Corporation (NASDAQ:SBUX) has been a popular stock among investors, with its shares trading at $80.00 as of Tuesday afternoon’s close. The company reported earnings per share in the last quarter that came out to be 0.01 cents on revenue totaling $3 billion.\n","Starbucks’ latest quarterly report showed an EPS loss of -1 cent compared to analysts’ estimates for a loss of 2 cents and sales coming in just below expectations by $100 million from analyst projections. However, this was still better than Wall Street forecasts which had called for losses of 4 cents per share and revenues falling short of consensus predictions\n"]}]},{"cell_type":"code","source":["save_path = \"/content/drive/MyDrive/HPML_Project/copy_unsloth_a100_3\"   # <— adjust if your folder is nested\n","\n","# 1) Save LoRA adapter + config\n","trainer.save_model(save_path)\n","\n","# 2) Save tokenizer files\n","tokenizer.save_pretrained(save_path)\n","\n","print(\" Saved adapters + tokenizer to\", save_path)"],"metadata":{"id":"4r2O9D3fxU-E","executionInfo":{"status":"ok","timestamp":1746589369314,"user_tz":240,"elapsed":1101,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7b79b1a8-a9ce-4a42-d25d-591f9d37ebd8"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":[" Saved adapters + tokenizer to /content/drive/MyDrive/HPML_Project/copy_unsloth_a100_3\n"]}]},{"cell_type":"markdown","source":["## Inference\n"],"metadata":{"id":"s23uOdZsCjLN"}},{"cell_type":"code","source":["# inference.py\n","\n","import torch\n","from peft import prepare_model_for_kbit_training, PeftModel\n","from unsloth import FastLanguageModel\n","\n","# 1) Load the same 4-bit base + tokenizer you fine-tuned on\n","base, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name     = \"unsloth/Llama-3.2-1B-bnb-4bit\",  # your base\n","    max_seq_length = 128,\n","    dtype          = torch.float16,                   # or None for auto\n","    load_in_4bit   = True,\n","    device_map     = \"auto\",\n",")\n","\n","# ensure pad/eos tokens are set\n","tokenizer.pad_token = tokenizer.eos_token\n","base.config.pad_token_id = tokenizer.pad_token_id\n","base.config.use_cache      = True\n","\n","# 2) Patch for QLoRA / k-bit adapters\n","base = prepare_model_for_kbit_training(base)\n","\n","# 3) Load your fine-tuned LoRA adapters\n","model = PeftModel.from_pretrained(\n","    base,\n","    \"/content/drive/MyDrive/HPML_Project/copy_unsloth_a100_3\",     # folder where you saved adapters + tokenizer\n","    device_map=\"auto\",          # shard onto GPU automatically\n",")\n","\n","# model.eval()\n","FastLanguageModel.for_inference(model)\n","\n","# 4) Inference helper\n","def answer(prompt: str,\n","           max_new_tokens: int = 128,\n","           temperature: float    = 0.2,\n","           top_p: float          = 0.7,\n","           repetition_penalty: float = 1.2,\n","           no_repeat_ngram_size: int = 3):\n","    # 1) tokenize\n","    inputs = tokenizer(\n","        prompt,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=512\n","    ).to(model.device)\n","\n","    input_ids = inputs[\"input_ids\"]\n","\n","    # 2) generate with anti‐repetition tweaks\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens       = max_new_tokens,\n","        temperature          = temperature,\n","        top_p                = top_p,\n","        do_sample            = True,\n","        repetition_penalty   = repetition_penalty,\n","        no_repeat_ngram_size = no_repeat_ngram_size,\n","        eos_token_id         = tokenizer.eos_token_id,\n","        pad_token_id         = tokenizer.pad_token_id,\n","        early_stopping       = True,\n","    )\n","\n","    # 3) strip off prompt‐tokens and decode only the new ones\n","    gen_ids = outputs[0][ input_ids.shape[-1] : ]\n","    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n","\n","# Try it out\n","print(answer(\"How is starbucks stock doing?\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VH9QPCYFCiJv","executionInfo":{"status":"ok","timestamp":1746589381985,"user_tz":240,"elapsed":10203,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"02bb04f1-d01c-4caf-8ae0-96dcfca5a812"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n","   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Starbucks (SBUX) has been a popular choice for investors looking to diversify their portfolio. The company’s strong performance and consistent growth have made it one of the most sought-after stocks in recent years.\n","However, with so many factors influencing its value, how can you tell if SBUX will continue to be a top pick or whether other options are better suited?\n","In this article, we’ll explore some key metrics that could help determine which direction SBUX might take next year – including earnings estimates from analysts who cover the stock as well as sentiment indicators like Zacks Rank & Price Performance Indexes such as Market Cap Growth Rate Indicator. We\n"]}]},{"cell_type":"code","source":["# inference.py\n","\n","import torch\n","from peft import prepare_model_for_kbit_training, PeftModel\n","from unsloth import FastLanguageModel\n","\n","# 1) Load the same 4-bit base + tokenizer you fine-tuned on\n","base, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name     = \"unsloth/Llama-3.2-1B-bnb-4bit\",  # your base\n","    max_seq_length = 128,\n","    dtype          = torch.float16,                   # or None for auto\n","    load_in_4bit   = True,\n","    device_map     = \"auto\",\n",")\n","\n","# ensure pad/eos tokens are set\n","tokenizer.pad_token = tokenizer.eos_token\n","base.config.pad_token_id = tokenizer.pad_token_id\n","base.config.use_cache      = True\n","\n","# 2) Patch for QLoRA / k-bit adapters\n","base = prepare_model_for_kbit_training(base)\n","\n","# 3) Load your fine-tuned LoRA adapters\n","model = PeftModel.from_pretrained(\n","    base,\n","    \"/content/drive/MyDrive/HPML_Project/copy_unsloth_a100_3\",     # folder where you saved adapters + tokenizer\n","    device_map=\"auto\",          # shard onto GPU automatically\n",")\n","\n","# model.eval()\n","FastLanguageModel.for_inference(model)\n","\n","# 4) Inference helper\n","def answer(prompt: str,\n","           max_new_tokens: int = 128,\n","           temperature: float    = 0.2,\n","           top_p: float          = 0.9,\n","           repetition_penalty: float = 1.2,\n","           no_repeat_ngram_size: int = 3):\n","    # 1) tokenize\n","    inputs = tokenizer(\n","        prompt,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=512\n","    ).to(model.device)\n","\n","    input_ids = inputs[\"input_ids\"]\n","\n","    # 2) generate with anti‐repetition tweaks\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens       = max_new_tokens,\n","        temperature          = temperature,\n","        top_p                = top_p,\n","        do_sample            = True,\n","        repetition_penalty   = repetition_penalty,\n","        no_repeat_ngram_size = no_repeat_ngram_size,\n","        eos_token_id         = tokenizer.eos_token_id,\n","        pad_token_id         = tokenizer.pad_token_id,\n","        early_stopping       = True,\n","    )\n","\n","    # 3) strip off prompt‐tokens and decode only the new ones\n","    gen_ids = outputs[0][ input_ids.shape[-1] : ]\n","    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n","\n","# Try it out\n","print(answer(\"What is the best performing stock?\"))"],"metadata":{"id":"djMnOyY1CyMv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746589424222,"user_tz":240,"elapsed":9268,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"4c3b7766-7975-4d19-e035-5f9cdccfd457"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n","   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","The answer to this question depends on your investment goals and risk tolerance. However, there are a few stocks that have consistently outperformed their peers in recent years: Apple (AAPL), Microsoft (MSFT), Amazon.com Inc.(AMZN). These companies all offer unique advantages for investors looking to maximize returns.\n","Apple’s strong performance can be attributed to its leadership position as one of the world’s largest technology giants with an extensive portfolio spanning mobile devices, software applications, cloud computing services, consumer electronics products like iPhones or iPads etc., along with other related businesses such as healthcare solutions through its acquisition of Fitbit Inc(FTT\n"]}]},{"cell_type":"code","source":["# inference.py\n","\n","import torch\n","from peft import prepare_model_for_kbit_training, PeftModel\n","from unsloth import FastLanguageModel\n","\n","# 1) Load the same 4-bit base + tokenizer you fine-tuned on\n","base, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name     = \"unsloth/Llama-3.2-1B-bnb-4bit\",  # your base\n","    max_seq_length = 128,\n","    dtype          = torch.float16,                   # or None for auto\n","    load_in_4bit   = True,\n","    device_map     = \"auto\",\n",")\n","\n","# ensure pad/eos tokens are set\n","tokenizer.pad_token = tokenizer.eos_token\n","base.config.pad_token_id = tokenizer.pad_token_id\n","base.config.use_cache      = True\n","\n","# 2) Patch for QLoRA / k-bit adapters\n","base = prepare_model_for_kbit_training(base)\n","\n","# 3) Load your fine-tuned LoRA adapters\n","model = PeftModel.from_pretrained(\n","    base,\n","    \"/content/drive/MyDrive/HPML_Project/copy_unsloth_a100_3\",     # folder where you saved adapters + tokenizer\n","    device_map=\"auto\",          # shard onto GPU automatically\n",")\n","\n","# model.eval()\n","FastLanguageModel.for_inference(model)\n","\n","# 4) Inference helper\n","def answer(prompt: str,\n","           max_new_tokens: int = 128,\n","           temperature: float    = 0.2,\n","           top_p: float          = 0.9,\n","           repetition_penalty: float = 1.2,\n","           no_repeat_ngram_size: int = 3):\n","    # 1) tokenize\n","    inputs = tokenizer(\n","        prompt,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=512\n","    ).to(model.device)\n","\n","    input_ids = inputs[\"input_ids\"]\n","\n","    # 2) generate with anti‐repetition tweaks\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens       = max_new_tokens,\n","        temperature          = temperature,\n","        top_p                = top_p,\n","        do_sample            = True,\n","        repetition_penalty   = repetition_penalty,\n","        no_repeat_ngram_size = no_repeat_ngram_size,\n","        eos_token_id         = tokenizer.eos_token_id,\n","        pad_token_id         = tokenizer.pad_token_id,\n","        early_stopping       = True,\n","    )\n","\n","    # 3) strip off prompt‐tokens and decode only the new ones\n","    gen_ids = outputs[0][ input_ids.shape[-1] : ]\n","    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n","\n","# Try it out\n","print(answer(\"What is the news on Lockheed Martin?\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FE_anQYVdGfv","executionInfo":{"status":"ok","timestamp":1746589433299,"user_tz":240,"elapsed":9075,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}},"outputId":"12d1669c-3685-4d55-a944-3a6e494b30ed"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n","   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","The company has been in a bit of trouble lately, with its stock price dropping by more than 10% over the past year. But what’s behind this decline and how can you make sense of it all?\n","Lockheed Martin (LMT) was founded back in 1912 as an aircraft manufacturer for the U.S military. Today they are one of America's largest defense contractors.\n","The company makes everything from missiles to drones – but their most famous product remains F-35 Lightning II fighter jets which have become known as \"the world's best\" due to their advanced technology capabilities.\n","In recent years however there has been some controversy surrounding\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"TKTjvdjwezAA","executionInfo":{"status":"aborted","timestamp":1746587515928,"user_tz":240,"elapsed":32510,"user":{"displayName":"Tanmay Bankar","userId":"15426054109741671763"}}},"execution_count":null,"outputs":[]}]}